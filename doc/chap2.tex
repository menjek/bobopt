\chapter{Static code analysis}
Detecting and repairing errors in code early in development process is important to reduce cost of repairing such errors. The most commonly used process to detect errors as soon as possible is code review. Static code analysis can be considered to be automated code review.

One of the biggest pitfalls of code review is its high price, when big portion of development budget consists of developer salaries. Two or more people read code looking for a way to improve it, finding and fixing errors or potential errors that can become real errors in future, performance issues, etc. Quality of code review also decreases with time spent reading code. Developers need to rest to increase quality.

Compiler warnings con be considered to be very basic static code analysis performed by compiler. It warns programmer about suspicious parts of code it detected in compilation. It is good practice to turn on all warnings inside compiler and compile code without any. The most commonly used compilers also provides switch to consider warnings as an errors. Software development companies have often rules to force programmers produce warning-less code or simply using mentioned compiler switch in development environment implicitly removing warnings.

But there's not much compilers can do in diagnostic errors. It's not their primary goal that is code compilation and more advanced diagnostic could hurt compilation times. In context of this thesis, where C++  is the analysed language, compilation times does really matter. Furthermore, there's no need to produce any binary when analysing code. Developer of tool for static code analysis would like to step in right after semantic analysis finished when there's enough \textit{understanding} of code. It's up to programmer whether he reuses front-end of compiler or implement his own.

\section{Pitfalls}
The fully C++ compliant, freeware and open sourced compilers\footnote{Both requirements are thesis requirements, proprietary compilers can do the job as well.} can be counted on one hand. Actually, if another requirement is being multiplatform, there are only two compilers left, \emph{GCC, the GNU Compiler Collection} and \emph{Clang/LLVM}. The lack of compilers is caused by complexity of C++ core language. Apart from standard syntax for procedural programming, C++ includes Turing-complete template metaprogramming language. Simplified, compiler needs to \textit{"execute"} code to generate code that will be eventually compiled. As an example of complexity, implementation of \code{export} feature was so huge task for compiler writers that this feature was eventually removed from language\footnote{Keyword has remained for future purpose.}. Next, the small number of C++ compilers caused by complexity of C++ is very strong argument not to try individually  implement own front-end.

Clang as a compiler front-end is covered by the whole following chapter, because it was chosen as implementation framework for tool. 

\section{GCC - the GNU Compiler Collection}
Why not just use GCC? Cite from Sparse FAQ [link] will partially cover the answer:\\

\emph{"Gcc is big, complex, and the gcc maintainers are not interested in other uses of the gcc front-end.  In fact, gcc has explicitly resisted splitting up the front and back ends and having some common intermediate language because of religious license issues - you can have multiple front ends and back ends, but they all have to be part of gcc and licensed under the GPL."}\\

The first sentence, especially the first few words, is the main reason programmers haven't started using GCC front-end to create tools for static code analysis.

Con's of GCC:

\begin{itemize}
\item It's very hard to learn for beginners.

\item Even though GCC consists of front-end, middle-end and back-end, it \textit{"feels"} monolithic by design. It's very difficult to decouple front and back ends.

\item GENERIC and GIMPLE\footnote{Names for different representations of AST. GIMPLE is subset of GENERIC for code optimizations.} representations of code are not as understandable as Clang AST.

\item GCC doesn't keep track about tokens location in source code. (e.g. It doesn't keep track of macro expansions so it's very difficult to refactor code with even a very little macro usage.)

\item Front-end optimizes code while parsing so AST doesn't correspond to source code (e.g. \code{x-x} is optimized to be \code{0}). It's extremely difficult to refactor such code.
\end{itemize}

Con of GCC in context of this thesis:

\begin{itemize}
\item Code base is written mainly in C language. Even though there's transition to C++, it's not going to change design of compiler. Transition will introduce only very few and simple C++ features. (e.g STL containers such as \code{std::vector}, partially replacing GCC internal garbage collection with smart pointers, templates, etc.)
\end{itemize}

\section{Elsa: The Elkhound-based C/C++ Parser}
Parser of C++ code created by very small group of people. It provides user with friendly AST representation of code that is designed to be easily extensible. Developer doesn't need to write single line of C++ code to extend AST. AST can be then traversed using either \emph{visitor design pattern} or manually traverse AST edges. The first way is used when analysis is mostly context insensitive. Both approaches are actually very similar to what Clang provides, with more details in the next chapter.

The biggest disadvantage of Elsa is that its development stopped long ago in 2005 when different project, called \emph{Oink}, that uses Elsa front-end started. Oink development then stopped before year 2011 when C++ experienced its \textit{"renaissance"}. Therefore there's almost no support for new C++11 language features. Oink as well as Elsa also doesn't have integrated preprocessor and it's extremely difficult to map AST with locations in source code. More advanced source-to-source transformations are therefore extremely difficult to do correctly. The last quiet important disadvantage is speed, which is not good enough for huge projects.

\section{VivaCore/OpenC++}
Library was created as basis for \emph{PSV-Studio} static code analyzer for C/C++ code. VivaCore is derived from older OpenC++ (OpenCxx) library. The idea of using OpenC++ appeared when team was implementing \emph{Viva64} library. They made a lot of changes to OpenC++ and because lack of resources they didn't continue to improve it\footnote{Many changes didn't fit into OpenC++ ideology so they would need to adapt and allocate new resources for such process.}, but rather developed their own library. Library became popular and is used as basis by other very popular tools such as  \emph{VisualAssist} by \emph{Whole Tomato Software}, \emph{Doxygen}, \emph{Gimpel Software PC-Lint}, \emph{Parasoft C++test} and more.

The first step of code analysis is to use preprocessor. Library uses external preprocessor what becomes its biggest disadvantage in source-to-source transformation process. Without integrated preprocessor it's next to impossible to track macro expansion and actual location of symbols in source code. Doxygen users may now understand why there's special \code{\textbackslash{def}} command to define macro documentation as it doesn't see macro usages because of using external preprocessor. VisualAssist uses Visual Studio preprocessor that feeds VivaCore library with preprocessed file. Therefore it often misses usage of symbols inside macros when renaming symbols.

Preprocessed input is then passed to library. There are two library subsystems that process code before it gets to lexical analysis. The first is input subsystem which has responsibility for putting preprocessed code into internal data structures. The second step is internally called \emph{Preprocessor}, but it doesn't preprocess input in the meaning of C++ preprocessor. It has mainly responsibility for two operations:

\begin{itemize}
\item Splitting code into strings and dividing them into two logical groups. One is for system libraries and the second one is for user code. Library user can choose whether he wants to analyze also system code or just user code.
\item Removing C++ non-related strings, specific for compiler (e.g. \code{SA\_Success} and \code{SA\_FormatString} are presented in Visual Studio headers.).
\end{itemize}

Next step is finally lexical analysis. Output of \emph{Lexer} can be used for basic metrics or syntax highlighting. VivaCore allows to quite easily modify set of tokens for lexical analysis.

VivaCore provides user with \emph{parse tree (PT)} called also \emph{derivation tree (DT)} as an output of syntactic analysis. Parse tree differs from abstract syntax tree in the way it contains nodes representing derivation rules used in syntactic analysis. The word \emph{abstract} came from the reasoning that it hides the way tree was constructed. It is actually possible to traverse PT as it was AST. Parse tree defines two basic sets of nodes with ancestors in \code{NonLeaf} and \code{Leaf} base classes which have \code{PTree}\footnote{\code{PTree} has \code{LightObject} as its base class used in GC.} class as their common ancestor declaring the only pure virtual member function:

\begin{lstlisting}[language=C++]
virtual bool IsLeaf() const = 0;
\end{lstlisting}

It is actually the only function that is needed to be defined in inherited classes letting their design to be more flexible.

Probably the most interesting part for library users is tree traversal. For this purpose there are three different \emph{walker} classes implemented

\begin{description}
\item[\code{Walker}] is responsible for walking over basic C++ constructions.
\item[\code{ClassWalker}] handles C++ class specific features.
\item[\code{ClasBodyWalker}] traverses body of C++ class.
\end{description}

It is possible to traverse PT multiple times so user can firstly traverse code for measurements and in the second traversal may modify PT. If user modifies tree nodes, it may happen that tree will be rebuilt.

\section{Related work}
Code quality in a huge projects is hard to maintain using code review since there's a lot of commits every day (e.g. Crysis 2 multiplayer had $\sim$100-150 code related commits every day collecting 103 different developers in its lifetime) and providing people resources for code review would be inefficient. Instead of that, companies use tools for static code analysis and diagnostic is further reviewed. Though not many companies trust tools in the way they let them do source-to-source transformations apart from formatting or simple refactoring.

\subsection{Clang Static Analyzer (\emph{clang-analyzer})}
Static analyzer that is part of the Clang project and therefore of course implemented on top of Clang tooling API. Analyzer is quite easily expandable by implementing \emph{checkers}, even though they may not be very intuitive. Authors created presentation where they try to explain \emph{"How To Write a Checker in 24 Hours"}[link]. They also demonstrate how to write simple checker for  Unix stream API. When writing checker, developer needs to understand how analyzer works under the hood.

Core of the analyzer does symbolic execution of code, exploring every possible path, tracking all variables and constructing \emph{Control Flow Graph (CFG)}. Checkers participate in CFG construction. Essentially checkers are visitors that react on specific set of events while traversing AST (e.g. \code{checkPreStmt}, \code{checkPostCall}, etc.) and eventually creating new CFG nodes. When they want to finish CFG exploration, they create \emph{sink} node. Checkers are stateless (i.e. visitor related member functions are defined as \code{const}) keeping their "state" data in \code{ProgramState} and its \emph{Generic Data Map (GDM)}.

Previous paragraphs described analyzer and checkers very briefly and it's hard to understand how checkers are exactly supposed to be implemented without example. On the other hand, mentioning CFG should give basic idea of what code problems analyzer aims to check. It's not that hard to check bad usage patterns in code using Clang tooling facilities such as AST visitor and matchers. The way harder problems are problems related to resource acquisition and release such as resource leaks or resource usage after release. The development manual page of analyzer contains very good advice according to implementation of checkers[link]:\\

\label{clang-analyzer-checkers}
\emph{"Some checks might not require path-sensitivity to be effective. Simple AST walk might be sufficient. If that is the case, consider implementing a Clang compiler warning. On the other hand, a check might not be acceptable as a compiler warning; for example, because of a relatively high false positive rate."}

\subsection{Clang Format (\emph{clang-format})}
Consistency in code formatting is very important in huge projects. It increases readability and code also becomes machine editable. Even though consistent code formatting is very important, there's not a lot of tools that supports automatic code formatting for C++ (e.g. \textit{BCPP}, \textit{Artistic Style}, \textit{Uncrustify}, \textit{GreatCode}, \emph{Style Revisor}, etc.)

The reason why companies allow use of an automatic formatting tools is that those tools guarantee they won't change code semantic (i.e. edit only white space characters, literals and comments) and thus won't break compilation. In this context there was proposal to let clang-format reorder includes, but it didn't get in because such change can break compilation. Main challenges for clang-format based on their design document:

\begin{itemize}
\item A vast number of different coding styles has evolved over time.
\item Macros need to be handled properly.
\item It should be possible to format code that is not yet syntactically correct.
\end{itemize}

It was a hard decision for clang-format authors whether they use Lexer or Parser to implement such tool. Both have their advantages and disadvantages in terms of performance, macro management, type info, etc. In the end they decided to stick with Lexer implementation, but there's still a discussion about adding also AST information. Though this discussion is leaning towards creating separate tool using AST, which already has the name, \emph{clang-tidy}.

\subsection{OCLint}
Another tool built on top of Clang LibTooling interface is \emph{OCLint}. Main parts of OCLint are \emph{Core}, \emph{Rules} and \emph{Reporter}.

Core controls the flow of analysis, dispatches tasks to another modules and outputs results. It parses code creating AST and provides modules with access to it. While parsing code it creates various metrics such as:

\begin{itemize}
\item Cyclomatic complexity.
\item NPath complexity.
\item Non commenting source statements.
\item Statement depth.
\end{itemize}

Rules may then provide \emph{RuleConfiguration} that defines limits for metrics. When limits  are exceeded, Core emits violation. There are two main approaches for modules to handle diagnostic:

\begin{description}
\item[Line based] is when modules are provided with lines of code.
\item[AST based] provides modules AST access using another two approaches  \footnote{Without further details. Very similar mechanisms will be mentioned in the following Clang chapter.}:
	\begin{itemize}
	\item Using \emph{Visitor pattern} to explore AST.
	\item Defining \emph{Matchers} for suspicious code patterns.
	\end{itemize}
\end{description}

Actually OCLint tries to create generic framework for code diagnostic. Modules are separated from Core code and can be loaded in runtime. Basic diagnostic can be really represented as set of code \textit{"bad usage"} patterns, where AST matchers become very comfortable mechanism. Reporting found \textit{"bad usage"} is the last task to be done.

From the negative side, pattern matching is not strong enough mechanism to catch even a little more complex errors such as resource leak. The other supported approaches than AST matchers don't really help more then just using Clang tooling API. It would be nice to remind advice from clang-analyzer developer manual (see Section \ref{clang-analyzer-checkers}).

\subsection{Scout}
The first, unfortunately also the last, tool that does also source-to-source transformations what create another dimension of problems regarding static code analysis. Scout is being developed in \emph{TU Dresden}, \emph{Center for Information Services and High Performance Computing}. It is supposed to do transformations for SIMD optimizations (e.g. loop vectorization), very similar task to what the most current compiler back-end optimizers do. It shall transform C code into optimized C code with compiler intrinsics. Naturally, auto-vectorization is done by compiler back-end optimizer, but there are limits what compiler can do. It needs to use extensive dependency and alias analysis to reason correctness of vectorization and often rejects more complex loops. Some compilers allow programmers to annotate loops with \code{pragma} directives giving responsibility for keeping some loop invariants to programmers. They can skip those checks before vectorization thus accepting more loops. Unfortunately, the measurement with specific Intel compiler using \code{pragma} directives gave insufficient results. For example compiler rejected loop vectorization after loop variable was changed from \code{unsigned int} to \code{signed int}. Actually also Scout provides semi-automatic vectorization, where programmers need to annotate loops using \code{pragma} directives to enable vectorization of given loop. 

Tool provides command line interface as well as graphical user interface. It is built over the Clang parser to build AST from C code. AST is then transformed to AST that represents optimized code and in the end transformed back to C code. Scout is implemented in the way it can be used in a built process. Tool is able to be configured. Configuration file is written in C++ code so there's no need to learn different configuration language. For example it is able to be configured with what set of intrinsics will be used (i.e. SSE2, SSE4, AVX, AVX2 or ARM NEON).

\subsubsection{Source-to-source transformation}
Possibilities of source-to-source transformations using the Clang API will be described in more details in the following chapter about Clang. Scout chose the approach of directly editing AST, approach that is not recommended by the Clang developers. It is work in progress to use probably the only correct approach using \code{TreeTransform} facility, but with lower priority because it just works with AST manipulation for now.

The central class for AST editing is called \code{StmtEditor}. Is is supposed to ease creation of new nodes and putting them together. What Clang provides is actually much more than just AST so node creation and insertion are complex operations which are supposed to be covered by this class. Scout implements them in naive way when there may exist usages that will create semantically invalid AST. Unless user knows how these member functions can be used, it should be fine to modify AST. Programmer then may inherit from \code{StmtEditor} class and use its member functions to manipulate with AST. After AST transformations are done, it should be passed back to semantic analysis. Currently in Scout it is not.

\subsection{Cppcheck}
The first tool in the list that doesn't use some compiler front-end as helper for parsing C++ code. CppCheck does this on its own, therefore quality of understanding code decreases accordingly. Input for checks is output of lexical analysis and it may be very difficult to implement more advanced checks. This does also mean that tool is not able to catch even syntactic errors. To at least simplify programmers life, CppCheck implements classes such as \code{Scope} or \code{SymbolDatabase} with functionality the names indicate.

Simplified version of CppCheck execution from programmers documentation[link]:

\begin{enumerate}
\item Parse command line.
\item \code{ThreadExecutor} creates needed \code{CppCheck} instances.
\item \code{CppCheck::check} is called for every file.
\item Inside \code{check}, preprocess file.
    \begin{itemize}
    \item Comments are removed.
    \item Macros are expanded.
    \end{itemize}
\item Tokenize file using \code{Tokenizer}.
\item Run all checks on tokenizer output called token list.
\item Simplify token list.\footnote{There are various simplifications applied to token list. Every simplification passes the whole token list looking for patterns and potentially changes this list. For example the first applied simplification changes \code{"string"[0]} to \code{'s'}. Another example is removing \code{std::} tokens from specific set of function calls.}
\item Run all checks on simplified token list.
\end{enumerate}
