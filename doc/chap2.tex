\chapter{Static code analysis}
Detection of errors in code early in a development process is important for reducing a development cost. The most commonly used process to detect errors as soon as possible is called \emph{code review}. A static code analysis can be considered to be an automated code review.

One of the biggest pitfalls of a code review is its high price. Two or more people read code looking for a way to improve it, finding and fixing errors or potential errors which can become real errors in the future, or performance issues. Quality of a code review decreases with time spent reading code. Developers need to rest to increase quality of their code review.

Compiler warnings can be considered to be a very basic static code analysis. It warns a programmer about suspicious parts of code it has detected in a compilation process. It is a good practice to turn on all compiler warnings and compile code without any detected. The most commonly used compilers provide a switch to consider warnings as errors. Software development companies often create rules to force programmers to produce warning-less code, or using the mentioned compiler switch in their development environment to implicitly remove compilation warnings.

A compiler cannot do much in diagnosing more complex errors. It is not their primary goal, which is code compilation, and more advanced diagnostic could increase compilation times. Furthermore, there is no need to produce any binary when analysing code. The most developers of a tool for static code analysis would like to access an output of the semantic analysis. It is up to a programmer whether he reuses a compiler front-end or implements his own.

\section{Pitfalls}
This chapter covers some of possible approaches for creating static code analysis tools. It also describes the implementation of some of popular analysis tools.

Due to complexity of the C++ core language, there are only very few fully C++ compliant, open-source and freeware\footnote{Proprietary compilers could do the job as well, but freeware compilers are preferred ones.} compilers. The two most known are \emph{GCC, the GNU Compiler Collection} \cite{gcc}, and \emph{Clang/LLVM} \cite{clang}. Apart from the standard syntax for procedural programming, C++ includes the Turing-complete template metaprogramming language. A compiler needs to \textit{execute} code in order to generate code which is eventually compiled into native code. As an example of language complexity, the implementation of the \emph{export} feature, which allows programmers to define templated code in one translation unit and use in different, was so huge task for compiler vendors\footnote{Only one compiler vendor was able to implement it, \emph{Comeau C/C++} \cite{comeau}.} that this feature was eventually removed from the language\footnote{The \code{export} keyword has remained in the language for a future purpose.}. Based on given facts, it would be extremely difficult and unwise to individually implement own C++ front-end.

\section{GCC - the GNU Compiler Collection}
GCC is a compiler with a 26 years old great history and is well established in the C++ software development world. Many helper tools for a build environment support GCC in some way, and yet programmers were struggling with writing static code analysis tools for C++. There are multiple reasons why programmers have not started using GCC. As an example, cite from \emph{Sparse FAQ} \cite{sparse} covers their reasons for avoiding GCC: \\

\textit{"Gcc is big, complex, and the gcc maintainers are not interested in other uses of the gcc front-end.  In fact, gcc has explicitly resisted splitting up the front and back ends and having some common intermediate language because of religious license issues - you can have multiple front ends and back ends, but they all have to be part of gcc and licensed under the GPL."}\\

The first sentence, especially the first few words, is the main reason programmers have not started using the GCC front-end to create tools for a static code analysis. Some of disadvantages in using GCC for tooling are:

\begin{itemize}
\item It is very hard to learn for beginners.

\item Even though GCC consists of front-end, middle-end and back-end, it \textit{feels} monolithic by design. It is very difficult to decouple front and back ends.

\item GENERIC and GIMPLE\footnote{GENERIC and GIMPLE are names for different representations of AST. GIMPLE is a subset of GENERIC for code optimizations.} representations of code are not intuitive.

\item GCC does not keep track of tokens locations in source code, e.g., it does not keep track of macro expansions. Therefore, it is very difficult to refactor code correctly.

\item Code is optimized when it is parsed so abstract syntax tree does not correspond to source code, e.g., \code{x-x} is optimized to be \code{0}. It is extremely difficult to refactor code based on such optimized abstract syntax tree.
\end{itemize}

One disadvantage that can be argued, but it is disadvantage for me personally.

\begin{itemize}
\item Code base is written mainly in the C language. Even though there is an ongoing transition to C++, it is not going to change the design of the compiler. This transition will introduce only very few and simple C++ features, e.g., STL containers, smart pointers to partially replace GCC internal garbage collection or templates.
\end{itemize}

\section{Elsa: The Elkhound-based C/C++ Parser}
Even a smaller group of developers is able to create a relatively nice C++ compiler front-end. \emph{Elsa} \cite{elsa} is such an example. It provides a programmer with a user-friendly AST representation of code, which is designed in the way it is easily extensible without writing a single line of C++ code. For the AST traversal, the front-end provides the mechanism designed using the \emph{visitor pattern}. The other way is to traverse a tree manually by following edges. The visitor pattern is useful for the context insensitive traversal.

The biggest disadvantage of Elsa is that its development stopped long time ago in 2005 when different project called \emph{Oink} \cite{oink} started. Oink uses the Elsa front-end. Later, the Oink development stopped before the year 2011 when C++ experienced its \textit{renaissance} with the latest approved standard, which introduced big changes to the core language and library. Therefore, there is almost no support for new C++11 language features. Oink as well as Elsa does not have an integrated preprocessor so it is extremely difficult to map AST with locations in source code. Elsa also suffers from lower speed, but it can be a negligible disadvantage for smaller projects.

\section{VivaCore/OpenC++}
The \emph{VivaCore} \cite{vivacore} library was created as basis for the \emph{PVS-Studio} \cite{pvs-studio} static code analyser for C/C++. The library is derived from the older \emph{OpenC++ (OpenCxx)} \cite{opencxx} library. The idea of using OpenC++ appeared when the team was implementing the \emph{Viva64} \cite{vivacore} library. They were making many changes to OpenC++ and because lack of resources they did not continue to improve it\footnote{Many changes did not fit into \textit{"general OpenC++ ideology"} \cite{vivacore-ideology} so they would need to adapt and allocate new resources for such process.}. They rather developed their own library. The VivaCore library has become popular. It has been used as basis by other popular tools such as  \emph{VisualAssist} by \emph{Whole Tomato Software}, \emph{Doxygen}, \emph{Gimpel Software PC-Lint}, \emph{Parasoft C++test} and more.

\begin{figure}[t!]
\centering
\vspace{0.5cm}
\begin{tikzpicture}[node distance=1.25cm, every text node part/.style={align=center}]
 \node(preprocessor) {External \\ preprocessor};
 \node(internal) [below=of preprocessor] {Preprocessor \\ subsystem};
 \node[below=of internal](lexer) {Lexer};
 \node[below=of lexer](parser) {Parser};
 \node[below=of parser](walker) {PT Walker};
 \node[below=of walker](metaprogram) {Metaprogram \\ subsystem};
 
 \path[<->] (preprocessor) edge node {} (internal);
 
 \path[*-*] (internal) edge node {} (lexer)
 	        (lexer) edge node {} (parser)
 	        (parser) edge node {} (walker)
 	        (walker) edge node {} (metaprogram);
 	        
 \node(utility) at (-5, -6.25) { C/C++ \\ Analysis \\ Library and \\ Utilities };
 
 \path[dashed] (utility) edge node {} (internal)
               (utility) edge node {} (lexer)
               (utility) edge node {} (parser)
               (utility) edge node {} (walker)
               (utility) edge node {} (metaprogram);
 
 \node(viva) at (-2.5, -11.5) {\LARGE\textbf{VivaCore}};
 
 \node[above=of internal, yshift=-1.25cm] (dummyfit){};
 
 \begin{pgfonlayer}{background}
  \node [draw,fit=(internal) (lexer) (parser) (walker) (metaprogram) (viva) (utility) (dummyfit)] {};
 \end{pgfonlayer}
 
\end{tikzpicture}
\caption{A design of the VivaCore library.}
\label{vivacore}
\end{figure}

The library uses an external preprocessor what becomes its biggest disadvantage in a source-to-source transformation process. Without an integrated preprocessor, it is extremely hard to track macro expansions and actual locations of symbols in source code.

A preprocessed input is passed to the library. Two library subsystems process code before it gets to the lexical analysis. The first is the input subsystem responsible for putting preprocessed code into internal data structures. The second subsystem is internally called \emph{Preprocessor}, but it does not preprocess an input in the meaning of the C++ preprocessor. It is responsible for two operations:

\begin{itemize}
\item Splitting code into strings and separating them into two logical groups. One group is for system libraries, the other group is for user code. A library user can choose whether he wants to analyse system code or just user code.
\item Removing compiler specific constructions not related to C/C++ languages, e.g., \code{SA\_Success} and \code{SA\_FormatString} are present in Visual Studio`s headers.
\end{itemize}

The next step is the lexical analysis. An output of \emph{Lexer} can be used for basic metrics or syntax highlighting. VivaCore allows modifications to the set of tokens for the lexical analysis.

VivaCore provides a user with \emph{parse tree (PT)}, called also \emph{derivation tree (DT)}, as an output of the syntactic analysis. Parse tree differs from abstract syntax tree in the way it contains nodes representing derivation rules used in the syntactic analysis. The word \emph{abstract} comes from the reasoning that the structure hides rules used in its construction. It is actually possible to traverse PT as if it was AST. VivaCore`s PT defines two basic sets of nodes with ancestors in \code{NonLeaf} and \code{Leaf} base classes, which have \code{PTree}\footnote{\code{PTree} has \code{LightObject} as its base class used in GC.} class as their common ancestor declaring the only pure virtual member function, see listing \ref{vivacore-node}. It is the only function necessary to be overridden in inherited classes letting their design to be more flexible.

\begin{lstlisting}[caption={The pure virtual member function of the base class for the VivaCore parse tree node.}, label={vivacore-node}]
virtual bool IsLeaf() const = 0;
\end{lstlisting}

Probably the most interesting part of the library interface is the tree traversal. There are three different \emph{walker} classes implemented for this purpose.

\begin{description}
\item[Walker] is responsible for walking over basic C++ constructions.
\item[ClassWalker] handles C++ class specific features.
\item[ClassBodyWalker] traverses a body of a C++ class.
\end{description}

It is possible to traverse PT multiple times. A user can traverse code for measurements at first. Later, in further traversals, he may modify PT. Modifying of tree nodes can trigger a three rebuild.

\section{Static code analysis tools}
Code quality in big projects is hard to maintain using a code review only. If there are many code related commits every day (e.g. \emph{Crysis 2} multiplayer had $\sim$100-150 code related commits every day collecting 130 different developers over the last year of the development \cite{crysis}), providing human resources for a code review would be inefficient. Instead of that, companies use tools for a static code analysis and diagnostic is further reviewed. However, not many companies trust tools enough to let them do source-to-source transformations, apart from formatting or simple refactoring.

\subsection{Clang Static Analyzer}
\label{clang-analyzer}
The static analyser is a part of the Clang project implemented on top of Clang tooling API. The analyser is quite easily extensible by implementing \emph{checkers}, even though their interface may not be intuitive. In the presentation called \textit{"How To Write a Checker in 24 Hours"} \cite{clang-analyzer-presentation},  authors demonstrate how to write a simple checker for  Unix stream API. When writing a checker, a developer needs to understand how the analyser works under the hood.

The core of the analyser does a symbolic execution of code, exploring every possible path, tracking all variables and constructing \emph{Control Flow Graph (CFG)}. Checkers participate in the CFG construction. Essentially, checkers are visitors that react on a specific set of events while traversing AST (e.g. \code{checkPreStmt}, \code{checkPostCall} functions) and eventually creating new CFG nodes. When they want to finish a CFG exploration, they create the \emph{sink} node. Checkers are stateless, i.e., visitor related member functions are defined as \code{const}, keeping their state data in \code{ProgramState} and its \emph{Generic Data Map (GDM)}.

The analyser aims to solve path-sensitive problems, e.g., problems related to the resource acquisition and release such as resource leaks and resource usage after release. The CFG construction is the core of such analysis. Actually, the development manual page of the analyser contains the important advice that discourages developers from implementing path-insensitive checkers \cite{clang-analyzer-manual}:\\

\label{clang-analyzer-checkers}
\emph{"Some checks might not require path-sensitivity to be effective. Simple AST walk might be sufficient. If that is the case, consider implementing a Clang compiler warning. On the other hand, a check might not be acceptable as a compiler warning; for example, because of a relatively high false positive rate."}

\subsection{Clang Format}
\label{clang-format}
A consistency in code formatting is very important in big projects. It increases a readability and code becomes better machine editable. Even though consistent code formatting is very important, there are not many tools that support automatic code formatting for C++, e.g., \emph{BCPP}, \emph{Artistic Style}, \emph{Uncrustify}, \emph{GreatCode}, \emph{Style Revisor}.

The reason why companies allow the usage of automatic formatting tools is that those tools guarantee they will not change the code semantic, i.e., they edit only white space characters, literals and comments. Therefore, they will not break a compilation. There was the proposal to let clang-format reorder file includes. It was not approved because such change can break a compilation. Main challenges for clang-format developers based on their design document were \cite{clang-format-design}:

\begin{itemize}
\item A vast number of different coding styles has evolved over time.
\item Macros need to be handled properly.
\item It should be possible to format code that is not yet syntactically correct.
\end{itemize}

It was the hard decision for clang-format developers whether they use lexer or parser to implement such tool. Both have their advantages and disadvantages in terms of a performance, macro management or type information. In the end, they have decided to retain with the implementation based on lexer, but there is still the ongoing discussion about adding AST information. However, this discussion is leaning towards creating a separate tool using AST, which already has the name \emph{clang-tidy} \cite{clang-tidy}.

\subsection{OCLint}
\emph{OCLint} \cite{oclint} is a tool implemented on top of the Clang tooling interface. It tries to create  a generic framework for code diagnostic. Main parts of OCLint are \emph{Core}, \emph{Rules} and \emph{Reporter}.

\emph{Core} controls a flow of the analysis, dispatches tasks to another modules and outputs results. It parses code, builds AST, and provides modules with an access to AST. While parsing code, it creates various metrics such as:

\begin{itemize}
\item Cyclomatic complexity.
\item NPath complexity.
\item Non-commenting source statements.
\item Statement depth.
\end{itemize}

\emph{Rules} can provide \emph{RuleConfiguration}, which defines limits for metrics. When limits are exceeded, \emph{Core} emits a violation. There are two main approaches for modules to handle diagnostic:

\begin{description}
\item[Line based] is when modules are provided with lines of code.
\item[AST based] provides modules with an access to AST using two approaches:
	\begin{itemize}
	\item Using \emph{visitor design pattern} to explore AST.
	\item Defining \emph{matchers} for suspicious code patterns.
	\end{itemize}
\end{description}

Modules are separated from \emph{Core} code, and they can be loaded in runtime. Basic diagnostic can be represented as a set of code patterns, and Clang AST matchers fits this purpose the best. The last task to do is to report a found diagnostic using \emph{Reporter}.

From the negative side, the pattern matching is not the strong enough mechanism to catch even a little more complex error such as resource leak. Other supported approaches than AST matchers do not really help more than just using the Clang tooling interface directly.

\subsection{Cppcheck}
An example of a tool that does not use any compiler front-end to process source code is Cppcheck \cite{cppcheck}. The tool does code parsing and analysis on its own, but quality of understanding of source code is lower than in well-established compiler front-ends. The input for code checks is the output of the lexical analysis. Thus, it can be very difficult to implement more advanced checks. The fact that the code analysis passes only the lexical analysis phase does also mean that the tool is not able to catch even syntactic errors. To ease programmer's life, Cppcheck implements classes such as \code{Scope} or \code{SymbolDatabase} with a functionality their names indicate.

The simplified version of a Cppcheck execution from the documentation for programmers can be written in eight steps \cite{cppcheck-doxygen}:

\begin{enumerate}
\item Parse the command line.
\item \code{ThreadExecutor} creates necessary \code{CppCheck} instances.
\item \code{CppCheck::check} processes every file.
\item Preprocess a file inside the \code{check} member function.
    \begin{itemize}
    \item Comments are removed.
    \item Macros are expanded.
    \end{itemize}
\item Tokenize a file using \code{Tokenizer}.
\item Run all checks on the \code{Tokenizer} output called a token list.
\item Simplify a token list.\footnote{There are various simplifications applied to a token list. Every simplification passes the whole token list looking for patterns and potentially changes this list. For example, the first applied simplification changes \code{"string"[0]} to \code{'s'}. Another example is removing of \code{std::} tokens from a specific set of function calls.}
\item Run all checks on a simplified token list.
\end{enumerate}

\section{Related work}
There are not many tools for front-end optimizations. The first reason is that it has been a hard task to implement any front-end tool. Furthermore, most optimizations are already implemented in compilers. This thesis aims to optimize the specific framework. With easier implementation of front-end tools, it is possible that there are already some optimization tools for specific frameworks, but there is no reason in making them public. Front-end optimizers for general code make sense, if they are very performance demanding and compiler cannot afford them.

\subsection{Scout}
\label{scout}
The front-end optimizer tool, called \emph{Scout} \cite{scout}, is being developed in \emph{TU Dresden}, \emph{Center for Information Services and High Performance Computing}. It is supposed to do transformations for front-end SIMD optimizations, e.g., loop auto-vectorization, very similar task to what most current compiler back-end optimizers do. It shall transform C code into optimized C code with compiler intrinsics. Naturally, auto-vectorization is done by a compiler back-end optimizer, but there are limits to what compiler can do. It needs to use the extensive dependency and alias analysis to reason correctness of vectorization and often rejects more complex loops. Some compilers allow programmers to annotate loops with \code{pragma} directives giving responsibility for keeping some loop invariants to programmers. A compiler can skip those checks before vectorization thus accepting more loops. Unfortunately, the measurement with the specific Intel compiler using pragma directives gave insufficient results. For example, the compiler rejected loop vectorization after the loop variable type was changed from \code{unsigned int} to \code{signed int}. Actually, Scout provides a semi-automatic vectorization, where programmers need to annotate loops using pragma directives to enable vectorization of a given loop. 

The tool provides a command line interface as well as graphical user interface. It uses Clang to build AST from C code. AST is then transformed to different AST that represents optimized code and transformed back to C code. The tool can be configured with the set of used intrinsics, i.e., \emph{SSE2}, \emph{SSE4}, \emph{AVX}, \emph{AVX2} or \emph{ARM NEON}.

\subsubsection{Source-to-source transformation}
Possibilities of source-to-source transformations using Clang tooling API are described in more details in the following chapter dedicated to Clang. Scout authors have decided to directly edit AST, the approach that is not recommended by Clang developers. It is work in progress to use \code{TreeTransform} facility, but with a lower priority because the manipulation with AST works for now.

The central class for AST editing is called \code{StmtEditor}. It is supposed to ease creation of new nodes and connecting them together. What Clang provides is actually much more than just AST so node creation and insertion are complex operations that are supposed to be covered by this class. Scout implements them in the naive way with possible usages that create invalid AST. As far as user knows how these member functions can be used, it should be fine to modify AST. A programmer is supposed to inherit from the \code{StmtEditor} class and use its member functions to manipulate with AST. After AST transformations are done, this transformed AST should be passed back for the semantic analysis. Currently, it does not happen in Scout.

\section{Summary}
The most mentioned tools for a static code analysis were implemented by a group of programmers. Without a good library for parsing C++ code, it is an extremely difficult task to implement any tool for an analysis of C++ code. The requirement for a code transformation makes the task even harder. The lack of the C++ tooling support has been generally mentioned as one of the biggest drawbacks of using the C++ language. The situation has changed and it is possible to implement a relatively complex tool for front-end optimizations individually or in a small group of people. The Scout tool is such example.